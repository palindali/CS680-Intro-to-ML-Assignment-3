\documentclass[10pt]{article}
\usepackage[left=1.5cm, right=1.5cm, top=0.8in, bottom=0.7in]{geometry} % lines=45, 
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{lastpage}
\usepackage[most,breakable]{tcolorbox}
\usepackage{pdfcol,xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,chains}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\usepackage{url}
\usepackage{dsfont}
\usepackage{amssymb,amsmath}
\usepackage{xspace}
\usepackage[normalem]{ulem}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage[breaklinks=true,colorlinks,linkcolor=magenta,urlcolor=magenta,citecolor=black]{hyperref}
\usepackage{cleveref}
\usepackage{xpatch}
\xpretocmd{\algorithm}{\hsize=\linewidth}{}{}

\newtcolorbox[auto counter]{exercise}[1][]{%
	colback=yellow!10,colframe=red!75!black,coltitle=white,use color stack,enforce breakable,enhanced,fonttitle=\bfseries,before upper={\parindent15pt\noindent}, title={\color{white}Exercise~\thetcbcounter: #1}}
\pagecolor{yellow!10}

\lhead{\textbf{University of Waterloo}}
\rhead{\textbf{2022 Fall}}
\chead{\textbf{CS480/680}}
\rfoot{\thepage/\pageref*{LastPage}}
\cfoot{\textbf{Yao-Liang Yu (yaoliang.yu@uwaterloo.ca) \textcopyright 2022}}

\newcommand{\pf}{\mathfrak{p}}
\newcommand{\qf}{\mathfrak{q}}
\newcommand{\pb}{\bar{p}}
\newcommand{\qb}{\bar{q}}
\newcommand{\pfb}{\bar{\mathfrak{p}}}
\newcommand{\qfb}{\bar{\mathfrak{q}}}
\newcommand{\rK}{\reflectbox{\ensuremath{K}}}

\newcommand{\bv}{\mathbf{b}}
\newcommand{\fv}{\mathbf{f}}
\newcommand{\gv}{\mathbf{g}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\gbs}{\bm{\mathsf{g}}}
\newcommand{\wbs}{\bm{\mathsf{w}}}
\newcommand{\xbs}{\bm{\mathsf{x}}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\Yv}{\mathbf{Y}}
\newcommand{\Bsf}{\mathsf{B}}
\newcommand{\Lsf}{\mathsf{L}}
\newcommand{\Xsf}{\mathsf{X}}
\newcommand{\Ysf}{\mathsf{Y}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\EE}{\mathds{E}}
\newcommand{\RR}{\mathds{R}}
\newcommand{\epsilonv}{\boldsymbol{\epsilon}}

\newcommand{\ans}[1]{{\color{blue}\textsf{Ans}: #1}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\dinner}[2]{\langle\!\langle#1,#2\rangle\!\rangle}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\one}{\mathbf{1}}
\newcommand{\pred}[1]{[\![#1]\!]}
\newcommand{\prox}[1]{\mathrm{P}_{#1}}
\newcommand{\sgm}{\mathsf{sgm}}
\newcommand{\sign}{\mathop{\mathrm{sign}}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\zero}{\mathbf{0}}

\newcommand{\ea}{{et al.}\xspace}
\newcommand{\eg}{{e.g.}\xspace}
\newcommand{\ie}{{i.e.}\xspace}
\newcommand{\iid}{{i.i.d.}\xspace}
\newcommand{\cf}{{cf.}\xspace}
\newcommand{\wrt}{{w.r.t.}\xspace}
\newcommand{\aka}{{a.k.a.}\xspace}
\newcommand{\etc}{{etc.}\xspace}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\magenta}[1]{{\color{magenta}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
%===========================================================
\begin{document}
	
	\begin{center}
		\large{\textbf{CS480/680: Introduction to Machine Learning} \\ Homework 3\\ \red{Due: 11:59 pm, November 19, 2022}, \red{submit on LEARN}.} \\
		
		{\bf \green{NAME}} \\
		{\bf \green{student number}}
		
	\end{center}
	
	\begin{center}
		Submit your writeup in pdf and all source code in a zip file (with proper documentation). Write a script for each programming exercise so that the TA can easily run and verify your results. Make sure your code runs!
		
		[Text in square brackets are hints that can be ignored.]
	\end{center}

	\begin{exercise}[CNN Implementation (8 pts)]
		\blue{\textbf{Note}: Please mention your Python version (and maybe the version of all other packages).}
		
		In this exercise you are going to run some experiments involving CNNs. You need to know \href{https://www.python.org/}{\magenta{Python}} and install the following libraries: \href{https://pytorch.org/get-started/locally/}{\magenta{Pytorch}}, \href{https://matplotlib.org/}{\magenta{matplotlib}} and all their dependencies. You can find detailed instructions and tutorials for each of these libraries on the respective websites. 
	
		For all experiments, running on CPU is sufficient. You do not need to run the code on GPUs, although you could, using for instance \href{https://colab.research.google.com/}{Google Colab}.
		Before start, we suggest you review what we learned about each layer in CNN, and read at least this \href{https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html}{\magenta{tutorial}}.
	
		\begin{enumerate}
			\item Implement and train a VGG11 net on the \href{https://pytorch.org/vision/stable/datasets.html#mnist}{\magenta{MNIST}} dataset. 
			VGG11 was an earlier version of VGG16 and can be found as model A in Table 1 of this \href{https://arxiv.org/pdf/1409.1556.pdf}{\magenta{paper}}, whose Section 2.1 also gives you all the details about each layer.
			The goal is to get the loss as close to 0 as possible. Note that our input dimension is different from the VGG paper. You need to resize each image in MNIST from its original size $28 \times 28$ to $32 \times 32$ [why?].
	
			For your convenience, we list the details of the VGG11 architecture here.
			The convolutional layers are denoted as \texttt{Conv(number of input channels, number of output channels, kernel size, stride, padding)};
			the batch normalization layers  are denoted as \texttt{BatchNorm(number of channels)};
			the max-pooling layers are denoted as \texttt{MaxPool(kernel size, stride)};
			the fully-connected layers are denoted as \texttt{FC(number of input features, number of output features)};
			the drop out layers are denoted as \texttt{Dropout(dropout ratio)}:
			\begin{verbatim}
			- Conv(001, 064, 3, 1, 1) - BatchNorm(064) - ReLU - MaxPool(2, 2)
			- Conv(064, 128, 3, 1, 1) - BatchNorm(128) - ReLU - MaxPool(2, 2)
			- Conv(128, 256, 3, 1, 1) - BatchNorm(256) - ReLU 
			- Conv(256, 256, 3, 1, 1) - BatchNorm(256) - ReLU - MaxPool(2, 2)
			- Conv(256, 512, 3, 1, 1) - BatchNorm(512) - ReLU 
			- Conv(512, 512, 3, 1, 1) - BatchNorm(512) - ReLU - MaxPool(2, 2)
			- Conv(512, 512, 3, 1, 1) - BatchNorm(512) - ReLU 
			- Conv(512, 512, 3, 1, 1) - BatchNorm(512) - ReLU - MaxPool(2, 2)
			- FC(0512, 4096) - ReLU - Dropout(0.5) 
			- FC(4096, 4096) - ReLU - Dropout(0.5) 
			- FC(4096, 10)
			\end{verbatim}
			You should use the \href{https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html}{cross-entropy loss} \verb|torch.nn.CrossEntropyLoss| at the end.
				
			[This experiment will take up to 1 hour on a CPU, so please be cautious of your time. If this running time is not bearable, you may cut the training set to 1/10, so only have $\sim$600 images per class instead of the regular $\sim$6000.]
			
			\item (4 pts) Once you've done the above, the next goal is to inspect the training process. \uline{Create the following plots}:
			\begin{enumerate}
				\item (1 pt) test accuracy vs the number of epochs (say 3 $\sim$ 5)
				\item (1 pt) training accuracy vs the number of epochs
				\item (1 pt) test loss vs the number of epochs
				\item (1 pt) training loss vs the number of epochs
			\end{enumerate}
			[If running more than 1 epoch is computationally infeasible, simply run 1 epoch and try to record the accuracy/loss after every few minibatches.]
			
			\ans{%
			\begin{center}
				\includegraphics[width=.4\textwidth]{example-image-a}\includegraphics[width=.4\textwidth]{example-image-b}\\
					\includegraphics[width=.4\textwidth]{example-image-c}\includegraphics[width=.4\textwidth]{example-image-a}\\
			\end{center}
			}
			
			\item Then, it is time to inspect the generalization properties of your final model. Flip and blur the \red{test set images} using any python library of your choice, and complete the following:		
			\begin{enumerate}[resume]
				\item (1 pt) test accuracy vs type of flip. Try the following two types of flipping: flip each image from left to right, and from top to bottom. \uline{Report the test accuracy after each flip. What is the effect?}
				
				You can read this \href{https://pytorch.org/vision/stable/transforms.html}{\magenta{doc}} to learn how to build a complex transformation pipeline. We suggest the following command for performing flipping: 
				\begin{verbatim}
				torchvision.transforms.RandomHorizontalFlip(p=1)
				torchvision.transforms.RandomVerticalFlip(p=1)
				\end{verbatim}
				
				\ans{We can see that 
				\begin{center}
				\includegraphics[width=.5\textwidth]{example-image-a}
				\end{center}
				}
						
				\item (1 pt) test accuracy vs Gaussian noise. Try adding standard Gaussian noise to each test image with variance 0.01, 0.1, 1 and \uline{report the test accuracies. What is the effect?}
				
				For instance, you may apply a user-defined lambda as a new transform t which adds Gaussian noise with variance say 0.01: 
				\begin{verbatim}
				t = torchvision.transforms.Lambda(lambda x : x + 0.1*torch.randn_like(x))
				\end{verbatim}
				
				\ans{We can see that
				\begin{center}
				\includegraphics[width=.5\textwidth]{example-image-b}
				\end{center}
				}			
			\end{enumerate} 
			
			\item (2 pts) Lastly, let us verify the effect of regularization. Retrain your model with data augmentation and test again as in item~3 above (both e and f). \uline{Report the test accuracy and explain} what kind of data augmentation you use in retraining.
			
				\ans{We can see that
				\begin{center}
				\includegraphics[width=.4\textwidth]{example-image-a}	\includegraphics[width=.4\textwidth]{example-image-b}
				\end{center}
				}		
		\end{enumerate}
	\end{exercise}

	\begin{exercise}[Graph Neural Networks (GNN) (8 pts)]
	You will need the following datasets to complete this exercise:
	\begin{itemize}
        \item Node classifications (small): \textbf{CiteSeer} 
            \begin{verbatim}
    from torch_geometric.datasets import Planetoid
    from torch_geometric.transforms import NormalizeFeatures
    dataset = Planetoid(root=`data/Planetoid', name=`CiteSeer', 
                transform=NormalizeFeatures())
            
    print(f`Dataset: {dataset}:')
    print(`======================')
    print(f`Number of graphs: {len(dataset)}')
    print(f`Number of features: {dataset.num_features}')
    print(f`Number of classes: {dataset.num_classes}')
            
    data = dataset[0]
    print(data)
            
    ## outputs:
    # Dataset: CiteSeer():
    # ======================
    # Number of graphs: 1
    # Number of features: 3703
    # Number of classes: 6
    # Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], 
                    val_mask=[3327], test_mask=[3327])
            \end{verbatim}
        
        \item Graph classification. \textbf{TUDataset/MUTAG} (Training using GIN takes 2s on cpu for $20$ epochs. Test Accuracy: $0.7895$ )
            \begin{verbatim}
    from torch_geometric.datasets import TUDataset
    from torch_geometric.transforms import NormalizeFeatures
    from torch_geometric.loader import DataLoader

    # dataset = KarateClub(transform=NormalizeFeatures())

    dataset = TUDataset(root=`data/TUDataset', name=`MUTAG',
                        transform=NormalizeFeatures())

    print(f`Dataset: {dataset}:')
    print(`======================')
    print(f`Number of graphs: {len(dataset)}')
    print(f`Number of features: {dataset.num_features}')
    print(f`Number of classes: {dataset.num_classes}')

    train_dataset = dataset[: int(len(dataset) * 0.8)]
    test_dataset = dataset[int(len(dataset) * 0.8): ]

    print(`==== train_dataset =====')
    print(train_dataset)

    print(`==== test_dataset =====')
    print(test_dataset)
            
    ## outputs:
    # Dataset: MUTAG(188):
    # ======================
    # Number of graphs: 188
    # Number of features: 7
    # Number of classes: 2
    # ==== train_dataset =====
    # MUTAG(150)
    # ==== test_dataset =====
    # MUTAG(38)
            \end{verbatim}
    \end{itemize}

    We recommend the following hyperparameter setups: 
    \begin{verbatim}
For all tasks: 

# hidden_dim: 64
# number of layers: 2
# activation: ReLU
# use Adam with learning rate = 0.01

For graph classification tasks, use # training epochs 30 would be sufficient 

For node classification tasks, use # training epochs 200 would be sufficient
    \end{verbatim}
    
		\begin{enumerate}
		    \item (node classification, 2 pts)
                Implement a GNN with backbone \textsf{torch\_geometric.nn.GCNConv} and test your model on CiteSeer. Report the following: 
                \begin{itemize}
                    \item \uline{plot the training loss and  classification error on training set} \wrt iteration 
                    
                    \item \uline{plot} the classification error on test set \wrt iteration
                    
                    \item \uline{visualize the last layer node embeddings of the initialized model and the trained model}. You may use the following code for visualization:
		            
	\begin{verbatim}
    %matplotlib inline
    import matplotlib.pyplot as plt
    from sklearn.manifold import TSNE
    import torch 

    # emb: (nNodes, hidden_dim) 
    # node_type: (nNodes,). Entries are torch.int64 ranged from 0 to num_class - 1
    def visualize(emb: torch.tensor, node_type: torch.tensor):
        z = TSNE(n_components=2).fit_transform(emb.detach().cpu().numpy())
        plt.figure(figsize=(10,10))
        plt.scatter(z[:, 0], z[:, 1], s=70, c=node_type, cmap="Set2")
        plt.show()
    \end{verbatim}

		      \ans{%
		        \begin{center}
		        \includegraphics[width=.45\textwidth]{example-image-a}\includegraphics[width=.45\textwidth]{example-image-b}
		         
                \includegraphics[width=.45\textwidth]{example-image-a}\includegraphics[width=.45\textwidth]{example-image-b}
                \end{center}
		        }
            \end{itemize}
		            
		  \item (node classification, 2 pts) \uline{Repeat the above task} with backbone \textsf{torch\_geometric.nn.GINConv}. 
		  
		  [\textbf{Note}: Training over CiteSeer does not take much time (less than 1 min on cpu).]

		  \ans{% 
          \begin{center}
          \includegraphics[width=.45\textwidth]{example-image-a}\includegraphics[width=.45\textwidth]{example-image-b}
          
          \includegraphics[width=.45\textwidth]{example-image-a}\includegraphics[width=.45\textwidth]{example-image-b}
          \end{center}
          }
		 
		 \item (graph classification, 2 pts) 
		 Implement a GNN with backbone \textsf{torch\_geometric.nn.GINConv}. \uline{Plot the training loss, classification error on the training set and classification error on the test set} \wrt iteration. (It is sufficient to implement the model with batch size one.) Include 1 figure for the two curves on training set and 1 figure for the curve on test set.
		 
		 \ans{%
		 
		 \includegraphics[width=.45\textwidth]{example-image-a}\includegraphics[width=.45\textwidth]{example-image-b}
		 }
		 
		 \item (graph classification with mini-batching, 2 pts) The graphs of a dataset may have different numbers of nodes, which makes it difficult to implement mini-batch by stacking node features of different graph samples. One workaround is to create a giant graph consisting of multiple isolated subgraphs, each associated to a graph sample. (See \href{https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html}{ADVANCED MINI-BATCHING} for details.) In this question, you need to modify your implementation to support mini-batching. \uline{Plot the training loss, classification error on training set and classification error on test set \wrt iteration}, for mini-batch size $4$, $8$ and $16$, respectively. Include 1 figure for each mini-batch size. 
		 
		 \ans{%
		 
		 \includegraphics[width=.32\textwidth]{example-image-a}\includegraphics[width=.32\textwidth]{example-image-b}
		 \includegraphics[width=.32\textwidth]{example-image-c}
		 }
		\end{enumerate}
	\end{exercise}

	\begin{exercise}[Regularization (4 pts)]
		\blue{\textbf{Notation}: For the vector $\xv_i$, we use $x_{ji}$ to denote its $j$-th element.}
		
		Overfitting to the training set is a big concern in machine learning. One simple remedy is through injecting noise: we randomly perturb each training data before feeding it into our machine learning algorithm. In this exercise you are going to prove that injecting noise to training data is essentially the same as adding some particular form of regularization. We use least-squares regression as an example, but the same idea extends to other models in machine learning almost effortlessly.
		
		Recall that least-squares regression aims at solving:
		\begin{align}
		\label{eq:lr}
		\min_{\wv\in \RR^d} ~ \sum_{i=1}^n (y_i - \wv^\top\xv_i )^2,
		\end{align}
		where $\xv_i \in \RR^d$ and $y_i \in \RR$ are the training data. (For simplicity, we omit the bias term here.) Now, instead of using the given feature vector $\xv_i$, we perturb it first by some independent noise $\epsilonv_i$ to get $\tilde{\xv}_i = f(\xv_i, \epsilonv_i)$, with different choices of the perturbation function $f$. Then, we solve the following \textbf{expected} least-squares regression problem:
		\begin{align}
		\label{eq:plr}
		\min_{\wv\in \RR^d} ~ \sum_{i=1}^n \mathbf{E}[(y_i - \wv^\top\tilde\xv_i )^2],
		\end{align}
		where the expectation removes the randomness in $\tilde \xv_i$ (due to the noise $\epsilonv_i$), and we treat $\xv_i, y_i$ as fixed here. [To understand the expectation, think of $n$ as so large that we have each data appearing repeatedly many times in our training set.]
		
		\begin{enumerate}
			\item (2 pts) Let $\tilde{\xv}_i = f(\xv_i, \epsilonv_i) = \xv_i + \epsilonv_i$ where $\epsilonv_i \sim \mathcal{N}(\zero, \lambda I)$ follows the standard Gaussian distribution. Simplify \eqref{eq:plr} as the usual least-squares regression \eqref{eq:lr}, plus a familiar regularization function on $\wv$.
			
			\ans{\vskip5cm
			}
				
			\item (2 pts) Let $\tilde{\xv}_i = f(\xv_i, \epsilonv_i) = \xv_i \odot \epsilonv_i$, where $\odot$ denotes the element-wise product and $p \epsilon_{ji} \sim \href{https://en.wikipedia.org/wiki/Bernoulli_distribution}{\magenta{\mathrm{Bernoulli}}}(p)$ \href{https://en.wikipedia.org/wiki/Independence_(probability_theory)}{\magenta{independently}} for each $j$. That is, with probability $1-p$ we reset $x_{ji} $ to 0 and with probability $p$ we scale $x_{ji}$ as $x_{ji}/p$. Note that for different training data $\xv_i$, $\epsilonv_i$'s are independent. Simplify \eqref{eq:plr} as the usual least-squares regression \eqref{eq:lr}, plus a different regularization function on $\wv$ (that may also depend on $\xv$). [This way of injecting noise, when applied to the weight vector $\wv$ in a neural network, is known as Dropout (DropConnect).]
	
		    \ans{\vskip8cm} 		
		\end{enumerate}
	\end{exercise}
\end{document}
              